{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT4212 Homework 4 \n",
    "## Search Engine Optimization and Analytics\n",
    "\n",
    "Term: Fall 2024\n",
    "\n",
    "*Individual Assignment, due Nov 3，23:59* \n",
    "\n",
    "## Submission Instruction\n",
    "\n",
    "This homework contains several coding tasks and short-answer questions to explore several predictive models for page-rank. For coding part, please write codes in the corresponding cells. I may provide some comment lines as guideline. For short-answers, type your answer in the cells with **ANSWER: HERE**. Please double click those cells and directly input your answer.\n",
    "\n",
    "I recommend you use Python 3 for this homework. Python 2 may not be supported. You can use either your own PC or Google Colab to do this homework. GPU support is NOT required.\n",
    "\n",
    "\n",
    "Save your notebook `.ipynb` file as `StudentID_YourName_HW4.ipynb`. Generate an `.html` file from `.ipynb` file and save as `StudentID_YourName_HW4.html`. Zip your notebook file and html file into a single `.zip` file. \n",
    "\n",
    "Upload the `zip` file as `StudentID_YourName_HW4.zip`. Please DO NOT include data file in your zip file (too large to upload and download).\n",
    "\n",
    "**Please make sure your code is executable.**\n",
    "\n",
    "If you are using Google Colab, \n",
    "* Please make sure you have expanded all hidden cells. You can refer to https://stackoverflow.com/questions/62457417/unhide-all-cells-in-google-colab for more information.\n",
    "* How to generate an HTML file from your notebook file in Google Colab? Please refer to https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab\n",
    "\n",
    "HW4 is worth 80 points in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels pandas numpy xgboost scikit-learn matplotlib --upgrade\n",
    "# use newest version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your name and studentID\n",
    "name = \"your name\"\n",
    "stuID = \"A0123456Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "# the recommended version is listed but you could try using the most updated one.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fix random seeds (Note that you may still get slightly different results.)\n",
    "np.random.seed(12345)\n",
    "\n",
    "# To ignore some warnings\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Information: About the Data\n",
    "The data set is acquired in the following steps.\n",
    "1. Identify a set of 20 keywords. \n",
    "2. For each keyword, search in google, and return the first 98 websites. It has $20\\times98=1960$ observations in total.\n",
    "3.  Split the dataset into training and test (or validation) data. The training data includes $70\\%$ of observations ($14\\times 98=1372$ rows) while the test one has $30\\%$ of observations ($6\\times 98=588$ rows). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two data files. `Train_dta.csv` for training data and `Test_dta.csv` for testing data. Open the data file with Excel may encounter some unexpected errors. Simply download another copy from Canvas, if it occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and test datasets and print the first 5 rows of the training dataset.\n",
    "# Please use pd.read_csv(\"data.csv\") to load your data\n",
    "# Run this cell\n",
    "\n",
    "feature_col =[\"TitleFlag\", \"TitleDensity\", \"URLFlag\", \"URLDensity\", \"MetaFlag\", \"MetaDensity\", \"PageAuthority\", \"DomainAuthority\", \"LinkingDomain\", \"InboundLink\" , \"RankingKeyword\" ]\n",
    "train_data = pd.read_csv(\"Train_dta.csv\") \n",
    "test_data = pd.read_csv(\"Test_dta.csv\") \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many columns in the data, e.g., title, url and meta desciptions. The detailed information about each column is in the appendix. We will only use: “TitleFlag”, “TitleDensity”, “URLFlag”, “URLDensity”, “MetaFlag”, “MetaDensity”, “PageAuthority”, “DomainAuthority”, “LinkingDomain”, “InboundLink” and “RankingKeyword” as features, **\"ReverseRank\" as label**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split feature and label\n",
    "# Run this cell\n",
    "\n",
    "train_feature = pd.read_csv(\"Train_dta.csv\",usecols=feature_col)\n",
    "train_label = pd.read_csv(\"Train_dta.csv\",usecols=[\"ReverseRank\"]) \n",
    "\n",
    "test_feature = pd.read_csv(\"Test_dta.csv\",usecols=feature_col)\n",
    "test_label = pd.read_csv(\"Test_dta.csv\",usecols=[\"ReverseRank\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "print(train_feature.shape,\n",
    "      train_label.shape,\n",
    "      test_feature.shape,\n",
    "      test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "# This serves as your data input\n",
    "X_train = train_feature # trainig feature, as a dataframe in pandas\n",
    "X_test = test_feature # test feature, as a df \n",
    "\n",
    "y_train = train_label.values # training label, as a numpy array\n",
    "y_test = test_label.values # test label, as a np array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid potential issues of shallow copy and deep copy. Try to load data separately for each problem, although they may be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 Pointwise Rank. (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q1. Linear Regression (10 points)\n",
    "Please use the training dataset to fit a linear regression with all variables aforementioned. Please use the trained linear regression model to predict the rank in the test dataset and **report the root mean square error (RMSE) of the prediction**, i.e., $\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$, where $n$ is the total number of data points in the test dataset, $y_i$ is the true value of the outcome variable in the test dataset, and $\\hat{y}_i$ is the model prediction of the outcome variable. (5 points)\n",
    "\n",
    "Reference: https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: USE sm.OLS(y_train, X_train)\n",
    "# Fit: USE model.fit()\n",
    "# y_train is a numpy array from train_label, X_train is a pandas dataframe from train_feature.\n",
    "# Remember to add an intercept by sm.add_constant() to both train and test data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary by model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print RMSE, y_test is a numpy array from test_label, y_pred is a numpy array from your model prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pick the most statistically significant variable and interpret its estimated beta coefficient.** (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ANSWER: HERE*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Logistic Regression (15 points)\n",
    "\n",
    "For the baseline logistic regression, it does not require an ordinal relationship among the levels of the outcome variable, e.g., level 1 does not necessarily imply superiority or inferiority compared with level 2. However, for ordinal variable, its levels can be ranked implying a higher value than other level, e.g., in the school grade, A is better than B. We will use **ordinal** logistic regression for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use the training dataset to fit an **ordinal** logistic regression model with all variables aforementioned. **Use the `ReverseRank` as the outcome variable**, that is apparently a ranked variable**. (5 points)\n",
    "\n",
    "Note that you may not be able to run ordinal logistic regression because the distributions of several variables are too skewed. You can use `new_variable = np.log(the_problematic_variable+1)` to **transform those variables in both training and testing data.** (5 points)\n",
    "\n",
    "Please use the trained ordinal logistic regression model to predict the rank in the test dataset. **Print the predicted rank on the test dataset and report the RMSE of the prediction.** （5 points） \n",
    "\n",
    "Reference: https://www.statsmodels.org/dev/examples/notebooks/generated/ordinal_regression.html.\n",
    "https://stats.oarc.ucla.edu/r/dae/ordinal-logistic-regression/. \n",
    "\n",
    "Remarks: You will find tons of useful materials about statistical modelling in UCLA website, even though mostly implemented with R or Stata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust your input\n",
    "# Please specify which parts of data you do a log transformation on.\n",
    "# There are many ways to determine the skewness, \n",
    "# e.g., plotting the distribution, calling pandas.DataFrame.skew, etc.\n",
    "# As long as you provide reasons for the transformation, you will be awarded the points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: use OrderedModel(y_train, X_train, distr='logit')\n",
    "# Fit: use model.fit(method='bfgs', disp=False)\n",
    "# y_train is a numpy array from train_label, X_train is a pandas dataframe from train_feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary by model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction: assume the predicted output of this model is y_pred, \n",
    "# for this question, you need to do y_pred.argmax(1)+3 to get the real output.\n",
    "# Please refer to the reference above for how to make prediction properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print RMSE, y_test is a numpy array from test_label, y_pred is a numpy array from your model prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the most statistically significant variable and interpret its estimated beta coefficient. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ANSWER: HERE*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Decision Tree (5 points)\n",
    "Please use the training dataset to fit a decision tree model with all variables aforementioned. **Use the `ReverseRank` as the target variable**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model & Fit: USE DecisionTreeRegressor(random_state=0).fit(...)\n",
    "# y_train is a numpy array from train_label, X_train is a pandas dataframe from train_feature.\n",
    "# Make prediction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print RMSE, y_test is a numpy array from test_label, y_pred is a numpy array from your model prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q4. Random Forest (5 points)\n",
    "Please use the training dataset to fit a random forest model with all variables aforementioned. **Use the `ReverseRank` as the target variable**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Input \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model & Fit: USE RandomForestRegressor(random_state=0).fit(...)\n",
    "# y_train is a numpy array from train_label, X_train is a pandas dataframe from train_feature.\n",
    "# Make prediction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print RMSE, y_test is a numpy array from test_label, y_pred is a numpy array from your model prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2. Pairwise Rank with `xgboost` (35 points)\n",
    "Please use the training dataset to fit an XGboost model with all variables aforementioned. **Use the `ReverseRank` as the target variable**. \n",
    "\n",
    "Show the feature importance plot. Use \"Gain\" for `measure` in the importance plot. \n",
    "\n",
    "For the XGboost prediction, please interpret the importance value of the most important variable. Please use the trained XGboost model to predict the rank in the test dataset and report the RMSE of the prediction. \n",
    "\n",
    "Reference: https://xgboost.readthedocs.io/en/latest/python/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Use `reg:linear` for `objective` and `rmse` for `eval_metric`. (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Parameter Setting \n",
    "# Use xgb.XGBRegressor\n",
    "# Use parameters below first and you can adjust it later.\n",
    "reg = xgb.XGBRegressor(max_depth = 6, \n",
    "                       eta = 0.1, \n",
    "                       gamma = 0.1,\n",
    "                       subsample = 0.8,\n",
    "                       colsample_bytree = 0.8,\n",
    "                       alpha = 0.5,\n",
    "                       objective = \"reg:linear\",\n",
    "                       eval_metric = \"rmse\",\n",
    "                       seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model by your_model.fit(X_train, y_train)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature importance plot. Use plot_importance(your_model, importance_type = 'gain')\n",
    "# If you don't see your plot, please run this cell again.\n",
    "plot_importance(reg, importance_type = 'gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please interpret the importance value of the most important variable.** (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ANSWER: HERE*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction: y_pred should be the output of your prediction, it cotains value\n",
    "# from 3 to 100 (may not be integers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print RMSE, y_test is a numpy array from test_label, y_pred is a numpy array from your model prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try different combinations of the hyper-parameters to get a lower RMSE.** (5 points)\n",
    "\n",
    "Please note that you can use any hyper-parameter tuning techniques in the lecture. The points are not awarded based on the exact number of RMSE. As long as it is lower than the original values and you can justify the technique you use, you will be awarded the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Redo Q1 by using `rank:pairwise` for `objective` and `rmse` for `eval_metric`. (15 points)\n",
    "Observe that we did not utilize the query information yet. However, in the dataset, we know that each query corresponds to 98 data points. Now we will leverage this information by setting group information for both training and testing data. (5 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Parameter Setting \n",
    "# Use xgb.XGBRanker\n",
    "# Use parameters below first and you can adjust it later.\n",
    "reg = xgb.XGBRanker(max_depth = 6, \n",
    "                           eta = 0.1, \n",
    "                           gamma = 0.1,\n",
    "                           subsample = 0.8,\n",
    "                           colsample_bytree = 0.8,\n",
    "                           alpha = 0.5,\n",
    "                           objective = \"rank:pairwise\",\n",
    "                           eval_metric = \"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model by your_model.fit(X_train, y_train, group = np.full(14,98))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature importance plot. Use plot_importance(your_model, importance_type = 'gain')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please interpret the importance value of the most important variable.** (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ANSWER: HERE*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here `y_pred` is *not* a direct rank on itself. We need to figure out how to get the rank from `y_pred`. Test data can be divided into 6 groups, we need to perform the following steps for each group.\n",
    "1. For each group, get `y_pred` from your model. \n",
    "2. The values in `y_pred` indicate a relative score for the rank. The higher the score, the better the page rank. For all 98 pages in one group, the page with the highest score should rank as 100, and the page with the lowest score should rank as 3. \n",
    "3. Repeat this for all groups. You are required to implement this by some simple codes. **y_pred_rank** is the predicted ranks you transformed from **y_pred**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction: y_pred is the predicted output from your model, y_pred_rank is the actual rank you get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print RMSE, y_test is a numpy array from test_label, y_pred_rank is a numpy array from your model prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try different combinations of the hyper-parameters to get a lower RMSE.** (5 points)\n",
    "\n",
    "Please note that you can use any hyper-parameter tuning techniques in the lecture. The points are not awarded based on the exact number of RMSE. As long as it is lower than the original values and you can justify the technique you use, you will be awarded the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Use `rank:ndcg` for `objective` and `rmse` for `eval_metric`. Redo Question 6. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Parameter Setting \n",
    "# Use xgb.XGBRanker\n",
    "# Use parameters below first and you can adjust it later.\n",
    "reg = xgb.XGBRanker(max_depth = 6, \n",
    "                           eta = 0.1, \n",
    "                           gamma = 0.1,\n",
    "                           subsample = 0.8,\n",
    "                           colsample_bytree = 0.8,\n",
    "                           alpha = 0.5,\n",
    "                           objective = \"rank:ndcg\",\n",
    "                           eval_metric = \"rmse\",\n",
    "                           ndcg_exp_gain = False) # please note that because we have more than 32 labels, we need to set ndcg_exp_gain = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost==1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model by your_model.fit(X_train, y_train, group = np.full(14,98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature importance plot. Use plot_importance(your_model, importance_type = 'gain')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please interpret the importance value of the most important variable.** (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ANSWER: HERE*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction: y_pred is the predicted output from your model, y_pred_rank is the actual rank you get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print RMSE, y_test is a numpy array from test_label, y_pred_rank is a numpy array from your model prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try different combinations of the hyper-parameters to get a lower RMSE.** (5 points)\n",
    "\n",
    "Please note that you can use any hyper-parameter tuning techniques in the lecture. The points are not awarded based on the exact number of RMSE. As long as it is lower than the original values and you can justify the technique you use, you will be awarded the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3. Interpretation Short-Answers (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please paste the RMSE values of all questions above in the table below. Please round your RMSE up to 4 decimals.\n",
    "\n",
    "To access the table written in markdown below, double click the placeholder table, edit the corresponding value and run the cell. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Tuning:\n",
    "\n",
    "| Question | P1-Q1 | P1-Q2 | P1-Q3 | P1-Q4 | P2-Q5 | P2-Q6 | P2-Q7 |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| RMSE | 1.0000 | 2.0000 | 3.0000 | 4.0000 | 5.0000 | 6.0000 | 7.0000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Tuning:\n",
    "\n",
    "| Question | P2-Q5 | P2-Q6 | P2-Q7 |\n",
    "| --- | --- | --- | --- | \n",
    "| RMSE |  5.0000 | 6.0000 | 7.0000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: Please describe what you observe from the RMSE value for all methods of pointwise ranking. What is the best method? Why? (5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: Please compare the results from pointwise ranking and pairwise ranking and describe your observatins.(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "1. ID: identification number (i.e., row number in the dataset)\n",
    "\n",
    "2. Position: the actual google ranking of the webpage to the query\n",
    "\n",
    "3. ReverseRank: equals $101$ minus Position. It is equivalent to the position. Sometimes using ReverseRank as the dependent variable can have a better prediction.\n",
    "\n",
    "4. Title: the title of the webpage\n",
    "\n",
    "5. URL: the URL of the webpage\n",
    "\n",
    "6. Meta: the meta description of the webpage\n",
    "\n",
    "7.\tTitleFlag: indicates that whether the whole keyword is included in the page title. TitleFlag equals 1 if yes, otherwise, 0.\n",
    "\n",
    "8.\tUrlFlag: indicates that whether the whole keyword is included in the page url. UrlFlag equals 1 if yes, otherwise, 0.\n",
    "\n",
    "9.\tMetaFlag: indicates that whether the whole keyword is included in the page meta description. MetaFlag equals 1 if yes, otherwise, 0.\n",
    "\n",
    "10.\tTitleDensity: is the percentage of times a keyword appears in the title of a web page compared to the total number of words in the title of a web page.\n",
    "\n",
    "11.\tUrlDensity: is the percentage of times a keyword appears in the URL of a web page compared to the total number of words in the URL of a web page.\n",
    "\n",
    "12.\tMetaDensity: is the percentage of times a keyword appears in the meta description of a web page compared to the total number of words in the meta description of a web page.\n",
    "\n",
    "13.\tPageAuthority: is a score developed by Moz that predicts how well a specific page will rank on search engine result pages (SERP). https://moz.com/learn/seo/page-authority\n",
    "\n",
    "14.\tDomainAuthority: is a search engine ranking score developed by Moz that predicts how well a website will rank on search engine result pages (SERPs). https://moz.com/learn/seo/domain-authority\n",
    "\n",
    "15.\tLinkingDomain: is the number of unique external domains linking to this page. Two or more links from the same websites are considered as one linking domain. Provided by Moz.\n",
    "\n",
    "16.\tInboundLink: is the number of unique external pages linking to this page. Two or more links from the same page on a website are considered as one inbound link. Provided by Moz.\n",
    "\n",
    "17.\tRankingKeyword: is the number of keywords for which this site ranks within the top 50 positions on Google US. Provided by Moz."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvnew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
